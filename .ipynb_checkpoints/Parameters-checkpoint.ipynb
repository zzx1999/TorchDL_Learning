{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "<class 'generator'>\n",
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n",
      "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))  # pytorch已进行默认初始化\n",
    "\n",
    "print(net)\n",
    "X = torch.rand(2, 4)\n",
    "Y = net(X).sum()\n",
    "#模型参数的访问、初始化和共享\n",
    "#访问模型参数——parameters() or named_parameters(),后者返回参数Tensor外还会返回其名字。\n",
    "print(type(net.named_parameters()))\n",
    "for name,param in net.named_parameters():\n",
    "    print(name,param.size())\n",
    "#也可以用索引得到特定某一层的索引——因为用的是Sequence\n",
    "for name,param in net[0].named_parameters():\n",
    "    print(name,param.size(),type(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight1\n"
     ]
    }
   ],
   "source": [
    "#另外返回的param的类型为torch.nn.parameter.Parameter，其实这是Tensor的子类，\n",
    "#和Tensor不同的是如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MyModel,self).__init__(**kwargs)\n",
    "        self.weight1 = nn.Parameter(torch.rand(20,20))\n",
    "        self.weight2 = torch.rand(20,20)\n",
    "    def forward(self,x):\n",
    "        pass\n",
    "n = MyModel()\n",
    "for name,param in n.named_parameters():\n",
    "    print(name)\n",
    "#因为Parameter是Tensor，即Tensor拥有的属性它都有，比如可以根据data来访问参数数值，用grad来访问参数梯度。\n",
    "#weight2不是Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[ 0.0056,  0.0040,  0.0132,  0.0030],\n",
      "        [ 0.0027, -0.0053,  0.0024,  0.0015],\n",
      "        [ 0.0033, -0.0124,  0.0017, -0.0112]])\n",
      "2.weight tensor([[-0.0097, -0.0001, -0.0042]])\n",
      "0.bias tensor([0., 0., 0.])\n",
      "2.bias tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "#初始化模型参数\n",
    "#PyTorch的init模块里提供了多种预设的初始化方法。\n",
    "#下面例子，将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并依然将偏差参数清零。\n",
    "for name,param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init.normal_(param,mean=0,std=0.01)\n",
    "        print(name,param.data)\n",
    "#用常数初始化权重\n",
    "for name,param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        init.constant_(param,val=0)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-0.0000, -6.8709,  9.9523,  7.2053],\n",
      "        [-0.0000,  9.5194,  9.1493, -5.6697],\n",
      "        [-0.0000,  0.0000, -7.0740, -0.0000]])\n",
      "2.weight tensor([[-0.0000, -9.6935, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "#自定义初始化方法\n",
    "#令权重有一半概率初始化为0，有另一半概率初始化为[−10,−5]和[5,10]两个区间里均匀分布的随机数。\n",
    "def init_weight_(tensor):\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10,10) #从均匀分布中取值——每个的概率是一样的\n",
    "        tensor *= (tensor.abs()>=5).float()\n",
    "for name,param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init_weight_(param)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#模型参数共享\n",
    "#1. Module类的forward函数里多次调用同一个层。Module类layer复用\n",
    "#2. 传入Sequential的模块是同一个Module实例的话参数也是共享的\n",
    "linear = nn.Linear(1,1,bias=False)\n",
    "net = nn.Sequential(linear,linear)\n",
    "print(net)\n",
    "for name,param in net.named_parameters():\n",
    "    init.constant_(param,val=3)\n",
    "    print(name,param.data)\n",
    "#在内存中，这两个线性层其实一个对象:\n",
    "print(id(net[0]) == id(net[1]))\n",
    "print(id(net[0].weight) == id(net[1].weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
