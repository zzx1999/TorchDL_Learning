{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#googlenet\n",
    "#Inception\n",
    "#Inception 块使用1*1卷积减少通道数从而降低模型复杂度\n",
    "import time\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2l.torch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    #c1 - c4 为每一条线路里的层的输出通道数,这些作为超参可以自己定义\n",
    "    def __init__(self,in_c,c1,c2,c3,c4):\n",
    "        super(Inception,self).__init__()\n",
    "        #线路1,单1*1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_c,c1,kernel_size=1)\n",
    "        #线路2,1*1卷积层后接3*3\n",
    "        self.p2_1 = nn.Conv2d(in_c,c2[0],kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0],c2[1],kernel_size=3,padding=1)\n",
    "        #线路3,1*1卷积层后接5*5\n",
    "        self.p3_1 = nn.Conv2d(in_c,c3[0],kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0],c3[1],kernel_size=5,padding=2)\n",
    "        #线路4,3*3最大池化接1*1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3,stride=1,padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_c,c4,kernel_size=1)\n",
    "    def forward(self,x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))\n",
    "        return torch.cat((p1,p2,p3,p4),dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    # 全局平均池化层，可将窗口设置成输入的宽高实现\n",
    "    #全局平均池化则直接把整幅feature maps（它的个数等于类别个数）进行平均池化，然后输入到softmax层中得到对应的每个类别的得分。\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d,self).__init__()\n",
    "    def forward(self,x):\n",
    "        return F.avg_pool2d(x,kernel_size=x.size()[2:])\n",
    "#第一模块使用一个64通道的7×7卷积层。\n",
    "b1 = nn.Sequential(nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=3,stride=2,padding=1))\n",
    "#第二模块使用2个卷积层：首先是64通道的1×1卷积层，然后是将通道增大3倍的3×3卷积层。\n",
    "b2 = nn.Sequential(nn.Conv2d(64,64,kernel_size=1),\n",
    "                   nn.Conv2d(64,192,kernel_size=3,padding=1),\n",
    "                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "                  )\n",
    "#第三模块串联2个完整的Inception块。\n",
    "##第一个输出通道数是63+128+32+32=256,第三第二分别将通道数减少到16/192=1/12,96/192=1/2\n",
    "b3 = nn.Sequential(Inception(192,64,(96,128),(16,32),32),\n",
    "                   Inception(256,128,(128,192),(32,96),64),#第二个是128+192+96+64=480\n",
    "                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "                  )\n",
    "#第四个模块串联了5个Inception\n",
    "b4 = nn.Sequential(Inception(480,192,(96,208),(16,48),64),#输出512\n",
    "                   Inception(512,160,(112,224),(24,64),64),#输出512\n",
    "                   Inception(512,128,(128,256),(24,64),64),#512\n",
    "                   Inception(512,112,(144,288),(32,64),64),#528\n",
    "                   Inception(528,256,(160,320),(32,128),128),#832\n",
    "                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))\n",
    "#第五个模块是串联了两个Inception\n",
    "b5 = nn.Sequential(Inception(832,256,(160,320),(32,128),128),#832\n",
    "                   Inception(832,384,(192,384),(48,128),128),#1024\n",
    "                   GlobalAvgPool2d())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([1, 64, 24, 24])\n",
      "output shape torch.Size([1, 192, 12, 12])\n",
      "output shape torch.Size([1, 480, 6, 6])\n",
      "output shape torch.Size([1, 832, 3, 3])\n",
      "output shape torch.Size([1, 1024, 1, 1])\n",
      "output shape torch.Size([1, 1024])\n",
      "output shape torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(b1,b2,b3,b4,b5,nn.Flatten(),nn.Linear(1024,1024))\n",
    "x = torch.rand(1,1,96,96)\n",
    "for blk in net.children():\n",
    "    x = blk(x)\n",
    "    print('output shape',x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "epoch 1, loss 2.3679, train acc 0.098, test acc 0.100, time 21.9 sec\n",
      "epoch 2, loss 2.3094, train acc 0.098, test acc 0.100, time 21.9 sec\n",
      "epoch 3, loss 2.3110, train acc 0.104, test acc 0.100, time 21.9 sec\n",
      "epoch 4, loss 2.3156, train acc 0.105, test acc 0.120, time 21.9 sec\n",
      "epoch 5, loss 0.8400, train acc 0.675, test acc 0.813, time 22.0 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size,resize=96)\n",
    "lr,num_epochs = 0.001,5\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。\n",
    "#批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定\n",
    "#对全连接层做批量归一化ϕ(BN(x)),其中x = Wu+b(仿射变化)\n",
    "#首先，对小批量B求均值和方差,B = {x_1,x_2,...,x_m},μ_B,σ_B^2\n",
    "#使用按元素开方和按元素除法对x(i)标准化:x^(i)←x(i)−μ_B/根号(σ_^B^2+ϵ),\n",
    "#批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 γ 和偏移（shift）参数 β。这两个参数和x(i)形状相同，皆为d维向量。\n",
    "#它们与x(i)分别做按元素乘法（符号⊙）和加法计算：\n",
    "\n",
    "#CNN \n",
    "#如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。\n",
    "#设小批量中有mm个样本。在单个通道上，假设卷积计算输出的高和宽分别为p和q。我们需要对该通道中m×p×q个元素同时做批量归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import torch\n",
    "# from torch import nn,optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "# import d2l.torch as d2l\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# def batch_norm(is_training,X,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "#     #判断当前模式是训练模式还是预测模式\n",
    "#     if not is_training:\n",
    "#         #预测模式\n",
    "#         x_hat = (X-moving_mean)/torch.sqrt(moving_var+eps) #直接用传进来的参数\n",
    "#     else:\n",
    "#         assert len(X.shape) in (2,4)\n",
    "#         if len(X.shape) == 2:\n",
    "#             #使用全连接层的情况,计算特征维度上的均值和方差\n",
    "#             mean = X.mean(dim=0)\n",
    "#             var = ((X - mean)**2).mean(dim=0)\n",
    "#         else:\n",
    "#             # 使用CNN(样本数,通道,宽,高),计算通道维度上(axis=1)的均值和方差,要保持X的形状方便以后做广播运算\n",
    "#             mean = X.mean(dim=0,keepdim=True).mean(dim=2,keepdim=True).mean(dim=3,keepdim=True)\n",
    "#             var = ((X-mean)**2).mean(dim=0,keepdim=True).mean(dim=2,keepdim=True).mean(dim=3,keepdim=True)\n",
    "#         #训练模式下用当前的均值和方差做标准化\n",
    "#         x_hat = (X-mean)/torch.sqrt(var+eps)\n",
    "#         #更新移动平均和均值方差\n",
    "#         moving_mean = momentum*moving_mean+(1.0-momentum)*mean\n",
    "#         moving_var = momentum*moving_var+(1.0-momentum)*var\n",
    "        \n",
    "#     Y = gamma * x_hat + beta #拉伸和便宜\n",
    "#     return Y,moving_mean,moving_var\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2l.torch as d2l\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况(样本数,通道,宽,高)，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #batch norm层自定义\n",
    "# #它保存参与求梯度和迭代的拉伸参数gamma和偏移参数beta，同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。\n",
    "# class BatchNorm(nn.Module):\n",
    "#     def __init__(self,num_features,num_dims):\n",
    "#         super(BatchNorm,self).__init__()\n",
    "#         if num_dims == 2:\n",
    "#             shape = (1,num_features)\n",
    "#         else:\n",
    "#             shape = (1,num_features,1,1)\n",
    "#         #参与求梯度和迭代的拉伸和偏移系数,分别初始化为0和1\n",
    "#         self.gamma = nn.Parameter(torch.ones(shape))\n",
    "#         self.beta = nn.Parameter(torch.zeros(shape))\n",
    "#         #不参与求梯度和迭代的变量\n",
    "#         self.moving_mean = torch.zeros(shape)\n",
    "#         self.moving_var = torch.zeros(shape)\n",
    "#     def forward(self,X):\n",
    "#         if self.moving_mean.device!=X.device:\n",
    "#             self.moving_mean = self.moving_mean.to(X.device)\n",
    "#             self.moving_var = self.moving_var.to(X.device)\n",
    "#         #保存更新过的moving_mean和moving_var\n",
    "#         Y,self.moving_mean,self.moving_var = batch_norm(self.training,X,self.gamma,self.beta,\n",
    "#                                                         self.moving_mean,self.moving_var,eps=1e-5,momentum=0.9)\n",
    "#         return Y\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training, \n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#批量归一化LeNet\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1,6,5),#in_channels,out_channels,kernel_size\n",
    "    BatchNorm(6,num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.Conv2d(6,16,5),\n",
    "    BatchNorm(16,num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16*4*4,120),\n",
    "    BatchNorm(120,num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120,84),\n",
    "    BatchNorm(84,num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84,10)\n",
    ")\n",
    "# class LeNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LeNet,self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(1,6,5),#in_channels,out_channels,kernel_size\n",
    "#             BatchNorm(6,num_dims=4),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.MaxPool2d(2,2),\n",
    "#             nn.Conv2d(6,16,5),\n",
    "#             BatchNorm(16,num_dims=4),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.MaxPool2d(2,2)\n",
    "#         )\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(16*4*4,120),\n",
    "#             BatchNorm(120,num_dims=2),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Linear(120,84),\n",
    "#             BatchNorm(84,num_dims=2),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Linear(84,10)\n",
    "#         )\n",
    "#     def forward(self,img):\n",
    "#         feature = self.conv(img)\n",
    "#         output = self.fc(feature.view(img.shape[0],-1))\n",
    "#         return output\n",
    "# net = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arena/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arena/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7776, train acc 0.816, test acc 0.806, time 3.8 sec\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [120, 84]], which is output 0 of TBackward, is at version 470; expected version 469 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5ec9d555f86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ch5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/D/zzx/TorchDL/d2l/torch.py\u001b[0m in \u001b[0;36mtrain_ch5\u001b[0;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epoches)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#梯度清零\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#算Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#反向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#更新权重\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [120, 84]], which is output 0 of TBackward, is at version 470; expected version 469 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "lr,num_epochs = 0.001,5\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            BatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            BatchNorm(16, num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            BatchNorm(120, num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            BatchNorm(84, num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:1\n",
      "epoch 1, loss 0.8004, train acc 0.804, test acc 0.832, time 3.9 sec\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [120, 84]], which is output 0 of TBackward, is at version 470; expected version 469 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bbafaf8833cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ch5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/D/zzx/TorchDL/d2l/torch.py\u001b[0m in \u001b[0;36mtrain_ch5\u001b[0;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epoches)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#梯度清零\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#算Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#反向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#更新权重\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [120, 84]], which is output 0 of TBackward, is at version 470; expected version 469 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\n",
    "#....算了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.BatchNorm1d(120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.BatchNorm1d(84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:1\n",
      "epoch 1, loss 0.9969, train acc 0.787, test acc 0.834, time 1.8 sec\n",
      "epoch 2, loss 0.5017, train acc 0.832, test acc 0.854, time 1.7 sec\n",
      "epoch 3, loss 0.3896, train acc 0.861, test acc 0.864, time 1.8 sec\n",
      "epoch 4, loss 0.3595, train acc 0.870, test acc 0.863, time 1.8 sec\n",
      "epoch 5, loss 0.3362, train acc 0.880, test acc 0.874, time 1.9 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resNet\n",
    "#残差块\n",
    "#首先有2个有相同输出通道数的3×3卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。\n",
    "#然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。\n",
    "#如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,use_oneConv=False,stride=1):\n",
    "        super(Residual,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1,stride=stride)\n",
    "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
    "        if use_oneConv:#使用1*1卷积\n",
    "            self.conv3 = nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self,X):\n",
    "        Y1 = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y2 = self.bn2(self.conv2(Y1))\n",
    "        if self.conv3:#是否要改变通道数\n",
    "            X = self.conv3(X)\n",
    "        return F.relu(Y2+X)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3,3)\n",
    "X = torch.rand((4,3,6,6))\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3,6,use_oneConv=True,stride=2)#可以通过1*1卷积增加通道数,用stride减半宽高\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet模型\n",
    "#跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。\n",
    "#不同之处在于ResNet每个卷积层后增加的批量归一化层。\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1,64,kernel_size=7,stride=2),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    ")\n",
    "#4个残差块组成的模块,每个模块使用若干个同样输出通道数的残差快\n",
    "#第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。\n",
    "#除了第一个模块 之后每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\n",
    "def resnet_block(in_channels,out_channels,num_residuals,first_block=False):\n",
    "    if first_block:\n",
    "        assert in_channels==out_channels #第一个模块通道数和输入通道一致\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if (i==0) and not first_block:#如果没有first\n",
    "            blk.append(Residual(in_channels,out_channels,use_oneConv=True,stride=2))\n",
    "        else:\n",
    "            blk.append(Residual(out_channels,out_channels))\n",
    "    return nn.Sequential(*blk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 output shape:\t torch.Size([1, 64, 109, 109])\n",
      "1 output shape:\t torch.Size([1, 64, 109, 109])\n",
      "2 output shape:\t torch.Size([1, 64, 109, 109])\n",
      "3 output shape:\t torch.Size([1, 64, 55, 55])\n",
      "resnet_block1 output shape:\t torch.Size([1, 64, 55, 55])\n",
      "resnet_block2 output shape:\t torch.Size([1, 128, 28, 28])\n",
      "resnet_block3 output shape:\t torch.Size([1, 256, 14, 14])\n",
      "resnet_block4 output shape:\t torch.Size([1, 512, 7, 7])\n",
      "global_avg_pool output shape:\t torch.Size([1, 512, 1, 1])\n",
      "fc output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net.add_module(\"resnet_block1\",resnet_block(64,64,2,first_block=True))\n",
    "net.add_module(\"resnet_block2\",resnet_block(64,128,2))#通道翻倍宽高减半\n",
    "net.add_module(\"resnet_block3\",resnet_block(128,256,2))\n",
    "net.add_module(\"resnet_block4\",resnet_block(256,512,2))\n",
    "net.add_module(\"global_avg_pool\",d2l.GlobalAvgPool2d())#(Batch,512,1,1)\n",
    "net.add_module(\"fc\",nn.Sequential(nn.Flatten(),nn.Linear(512,10)))\n",
    "#每个Residual有两个Conv(不算1*1),每个block共4个,加上最开始的卷积和最后的fc一共是18层,ResNet18\n",
    "X = torch.rand((1,1,224,224))\n",
    "for name ,layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:1\n",
      "epoch 1, loss 0.4185, train acc 0.846, test acc 0.879, time 17.4 sec\n",
      "epoch 2, loss 1.6600, train acc 0.398, test acc 0.668, time 17.2 sec\n",
      "epoch 3, loss 0.6397, train acc 0.754, test acc 0.789, time 17.3 sec\n",
      "epoch 4, loss 0.4653, train acc 0.823, test acc 0.832, time 17.4 sec\n",
      "epoch 5, loss 0.3657, train acc 0.863, test acc 0.872, time 17.5 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size,resize=96)\n",
    "lr,num_epochs = 0.001,5\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DenseNet\n",
    "#DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是在通道维上连结。这样模块A的输出可以直接传入模块B后面的层。\n",
    "#DenseNet的主要构建模块是稠密块（dense block）和过渡层（transition layer）。\n",
    "#前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。\n",
    "def conv_blk(in_channels,out_channels):\n",
    "    blk = nn.Sequential(nn.BatchNorm2d(in_channels),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#稠密块由多个conv_block组成，每块使用相同的输出通道数。但在前向计算时，我们将每块的输入和输出在通道维上连结。\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self,num_convs,in_channels,out_channels):\n",
    "        super(DenseBlock,self).__init__()\n",
    "        net = []\n",
    "        for i in range(num_convs):\n",
    "            in_c = in_channels+i*out_channels\n",
    "            net.append(conv_blk(in_c,out_channels))\n",
    "        self.net = nn.ModuleList(net)\n",
    "        self.out_channels = in_channels + num_convs * out_channels\n",
    "    def forward(self,X):\n",
    "        for blk in self.net:\n",
    "           Y = blk(X)\n",
    "           X = torch.cat((X,Y),dim=1)#通道维上将输入和输出连接\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(2,3,10)\n",
    "X = torch.rand(4,3,8,8)\n",
    "Y = blk(X)\n",
    "Y.shape#3+2*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#过渡层\n",
    "#过渡层用来控制模型复杂度。它通过1×1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。\n",
    "def transition_block(in_channels,out_channels):\n",
    "    blk = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels,out_channels,kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "    )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(23,10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DenseNet模型\n",
    "#DenseNet首先使用同ResNet一样的单卷积层和最大池化层。\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    ")\n",
    "#DenseNet使用的是4个稠密块。同ResNet一样，我们可以设置每个稠密块使用多少个卷积层。\n",
    "#这里我们设成4，从而与上一节的ResNet-18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。\n",
    "num_channels,grow_rate = 64,32\n",
    "num_convs_in_dense_blk = [4,4,4,4]\n",
    "\n",
    "for i,num_convs in enumerate(num_convs_in_dense_blk):\n",
    "    DB = DenseBlock(num_convs,num_channels,grow_rate)\n",
    "    net.add_module(\"DenseBlock_%d\" % i,DB)\n",
    "    #上一个稠密块输出的通道数\n",
    "    num_channels = DB.out_channels\n",
    "    #在稠密块上加上通道减半的过渡层\n",
    "    if i != len(num_convs_in_dense_blk)-1:#如果没到最后一层\n",
    "        net.add_module(\"transition_%d\" % i,transition_block(num_channels,num_channels//2))\n",
    "        num_channels = num_channels // 2\n",
    "#和ResNet一样最后接上全局池化层\n",
    "net.add_module(\"BN\",nn.BatchNorm2d(num_channels))\n",
    "net.add_module(\"relu\",nn.ReLU())\n",
    "net.add_module(\"global_avg_pool\",d2l.GlobalAvgPool2d())\n",
    "net.add_module(\"fc\",nn.Sequential(nn.Flatten(),nn.Linear(num_channels,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 output shape\t torch.Size([1, 64, 48, 48])\n",
      "1 output shape\t torch.Size([1, 64, 48, 48])\n",
      "2 output shape\t torch.Size([1, 64, 48, 48])\n",
      "3 output shape\t torch.Size([1, 64, 24, 24])\n",
      "DenseBlock_0 output shape\t torch.Size([1, 192, 24, 24])\n",
      "transition_0 output shape\t torch.Size([1, 96, 12, 12])\n",
      "DenseBlock_1 output shape\t torch.Size([1, 224, 12, 12])\n",
      "transition_1 output shape\t torch.Size([1, 112, 6, 6])\n",
      "DenseBlock_2 output shape\t torch.Size([1, 240, 6, 6])\n",
      "transition_2 output shape\t torch.Size([1, 120, 3, 3])\n",
      "DenseBlock_3 output shape\t torch.Size([1, 248, 3, 3])\n",
      "BN output shape\t torch.Size([1, 248, 3, 3])\n",
      "relu output shape\t torch.Size([1, 248, 3, 3])\n",
      "global_avg_pool output shape\t torch.Size([1, 248, 1, 1])\n",
      "fc output shape\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((1,1,96,96))\n",
    "#打印每层信息\n",
    "for name,layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name,'output shape\\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size,resize=96)\n",
    "lr,num_epochs = 0.001,5\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
