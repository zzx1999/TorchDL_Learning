{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2D()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#卷积神经网络CNN\n",
    "#二维卷积\n",
    "#互相关计算(cross-correlation)，输入二维数组和卷积核对应相乘并求和\n",
    "#卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。\n",
    "#二维卷积层\n",
    "import torch\n",
    "from torch import nn\n",
    "import d2l.torch as d2l\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self,kernelsize):\n",
    "        super(Conv2D,self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernelsize))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "    def forward(self,x):\n",
    "        return d2l.corr2d(x,self.weight)+self.bias\n",
    "X = torch.tensor([[0,1,2],[3,4,5],[6,7,8]])\n",
    "net = Conv2D((2,2))\n",
    "net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#图像物体边缘检测\n",
    "X = torch.ones(6,8)\n",
    "X[:,2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.tensor([[1,-1]]) #如果横向相邻元素相同，输出为0；否则输出为非0。\n",
    "Y = d2l.corr2d(X,K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 ,loss 8.531\n",
      "Step 10 ,loss 2.260\n",
      "Step 15 ,loss 0.616\n",
      "Step 20 ,loss 0.170\n"
     ]
    }
   ],
   "source": [
    "#通过数据学习核数组，用上面的例子使用 输入数据X和输出数据Y来学习出K\n",
    "conv2d = Conv2D(kernelsize=(1,2))\n",
    "step = 20\n",
    "lr = 0.01\n",
    "for i in range(step):\n",
    "    y_hat = conv2d(X)\n",
    "    l = ((y_hat-Y)**2).sum()\n",
    "    l.backward()\n",
    "    \n",
    "    #梯度下降\n",
    "    conv2d.weight.data -= lr * conv2d.weight.grad\n",
    "    conv2d.bias.data -= lr * conv2d.bias.grad\n",
    "    \n",
    "    #梯度清零\n",
    "    conv2d.weight.grad.fill_(0)\n",
    "    conv2d.bias.grad.fill_(0)\n",
    "    \n",
    "    if (i+1)%5==0:\n",
    "        print('Step %d ,loss %.3f' % (i+1,l.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#卷积运算\n",
    "#为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。\n",
    "#二维卷积层输出的二维数组——特征图，输入在空间维度上的某种表征\n",
    "#影响x的前向传播所有可能的输入区域叫x的感受野，网络越深单个元素感受野更加广阔\n",
    "\n",
    "#填充和步幅\n",
    "#如果没有填充和步幅度输出的大小是：（nh-kh+1)×(nw-kW+1)，输入大小：nh*nw，输出大小是:kh*kw\n",
    "#填充——填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。\n",
    "#那么输出大小变成：（nh-kh+ph+1)×(nw-kw+pw+1),在高的两侧一共填充ph行，在宽的两侧一共填充pw列\n",
    "#一般情况下会设置ph=kh−1和pw=kw−1来使输入和输出具有相同的高和宽。\n",
    "import torch\n",
    "from torch import nn\n",
    "import d2l.torch as d2l\n",
    "#定义一个函数来计算卷积层,它对输入和输出做相应的升维和降维\n",
    "def comp_conv2d(conv2d,X):\n",
    "    #(1,1)代表批量大小和通道数\n",
    "    X = X.view((1,1)+X.shape)\n",
    "    Y = conv2d(X)\n",
    "    return Y.view(Y.shape[2:]) # 排除不关心的前两维：批量和通道\n",
    "#创建一个高和宽为3的二维卷积层，然后设输入高和宽两侧的填充数分别为1。\n",
    "conv2d = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=3,padding=1)\n",
    "X = torch.randn(8,8)\n",
    "comp_conv2d(conv2d,X).shape\n",
    "#当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。\n",
    "conv2d = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(5,3),padding=(2,1)) #第一个是左右都填充2列，第二个是上下都填充1行\n",
    "X = torch.randn(8,8)\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#步幅——每次滑动的行数和列数称为步幅（stride）。\n",
    "#一般来说，当高上步幅为sh，宽上步幅为sw时，输出形状为⌊(nh−kh+ph+sh)/sh⌋×⌊(nw−kw+pw+sw)/sw⌋.——向上取整\n",
    "#将上面的例子步幅调成2，那么输出的宽高减半\n",
    "\n",
    "conv2d = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(5,3),padding=(2,1),stride=2) #第一个是左右都填充2列，第二个是上下都填充1行\n",
    "X = torch.randn(8,8)\n",
    "comp_conv2d(conv2d,X).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(3,5),padding=(0,1),stride=(3,4))\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#多输入输出通道\n",
    "#多输入通道\n",
    "#由于输入和卷积核各有ci个通道，我们可以在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，\n",
    "#再将这ci个互相关运算的二维输出按通道相加，得到一个二维数组。\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "def corr2d_multi_in(X,K):\n",
    "    res = d2l.corr2d(X[0,:,:],K[0,:,:])\n",
    "    for i in range(1,X.shape[0]):\n",
    "        res+=d2l.corr2d(X[i,:,:],K[i,:,:])\n",
    "    return res\n",
    "X = torch.tensor([[[0,1,2],[3,4,5],[6,7,8]],[[1,2,3],[4,5,6],[7,8,9]]])\n",
    "K = torch.tensor([[[0,1],[2,3]],[[1,2],[3,4]]])\n",
    "corr2d_multi_in(X,K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#多输出通道\n",
    "#如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为ci×kh×kw的核数组。将它们在输出通道维上连结，\n",
    "#卷积核的形状即co×ci×kh×kw。\n",
    "def corr2d_multi_in_out(X,K):\n",
    "    return torch.stack([corr2d_multi_in(X,k) for k in K])\n",
    "\n",
    "K = torch.stack([K,K+1,K+2]) #把数组K和K+1（K中元素加1），K+2链接起来\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1*1卷积\n",
    "#输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。\n",
    "#假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×1卷积层的作用与全连接层等价。\n",
    "#1*1卷积实现\n",
    "def corr2d_multi_in_out_1(X,K):\n",
    "    c_i,h,w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.view(c_i,h*w)\n",
    "    K = K.view(c_o,c_i)\n",
    "    Y = torch.mm(K,X) #全连接层矩阵乘法\n",
    "    return Y.view(c_o,h,w)\n",
    "X = torch.rand(3,3,3)\n",
    "K = torch.rand(2,3,1,1)\n",
    "Y1 = corr2d_multi_in_out_1(X,K)\n",
    "Y2 = corr2d_multi_in_out(X,K)\n",
    "(Y1-Y2).norm().item()<1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1×1卷积层被当作保持高和宽维度形状不变的全连接层使用。于是，我们可以通过调整网络层之间的通道数来控制模型复杂度。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
