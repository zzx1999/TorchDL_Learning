{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.1877e-38, -2.1524e-10, -2.1876e+02],\n",
      "        [ 4.1877e-38, -2.1524e-10, -8.7507e+02],\n",
      "        [ 4.1877e-38, -2.1525e-10, -3.5003e+03],\n",
      "        [ 4.1877e-38, -2.1525e-10, -1.8083e-22],\n",
      "        [ 4.1877e-38, -2.1525e-10, -1.4002e+04]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5,3) #创建张量\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3821, 0.8112, 0.7124],\n",
      "        [0.8856, 0.5740, 0.9300],\n",
      "        [0.4547, 0.1474, 0.9945],\n",
      "        [0.5063, 0.3456, 0.6025],\n",
      "        [0.1465, 0.7926, 0.4192]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3) #随机张量\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5,3,dtype = torch.long) #创建全为0的张量,数据类型为long\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.3776,  0.0080,  1.3954],\n",
      "        [ 0.5815, -0.2561,  2.3722],\n",
      "        [ 0.9675,  0.0134, -1.4088],\n",
      "        [-1.5676, -0.4853, -0.8766],\n",
      "        [-0.3781,  0.1249,  0.4319]])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5,3]) #根据数据创建\n",
    "print(x)\n",
    "x = x.new_ones(5,3,dtype = torch.float64)\n",
    "print(x)\n",
    "x = torch.randn_like(x,dtype=torch.float) #指定新的数据类型，randn：标准分布，rand:均匀分布，normal(mean,std)：正态分布\n",
    "print(x)\n",
    "print(x.size())\n",
    "print(x.shape) #返回tuple类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2906,  0.1496,  2.2279],\n",
      "        [ 1.3803,  0.6473,  2.4782],\n",
      "        [ 0.9698,  0.9211, -1.0006],\n",
      "        [-1.4828,  0.4075, -0.1841],\n",
      "        [-0.1305,  0.2608,  0.7563]])\n",
      "tensor([[ 0.2906,  0.1496,  2.2279],\n",
      "        [ 1.3803,  0.6473,  2.4782],\n",
      "        [ 0.9698,  0.9211, -1.0006],\n",
      "        [-1.4828,  0.4075, -0.1841],\n",
      "        [-0.1305,  0.2608,  0.7563]])\n",
      "tensor([[ 0.2906,  0.1496,  2.2279],\n",
      "        [ 1.3803,  0.6473,  2.4782],\n",
      "        [ 0.9698,  0.9211, -1.0006],\n",
      "        [-1.4828,  0.4075, -0.1841],\n",
      "        [-0.1305,  0.2608,  0.7563]])\n"
     ]
    }
   ],
   "source": [
    "#加法\n",
    "y = torch.rand(5,3)\n",
    "print(x+y)\n",
    "print(x.add(y)) #如果需要执行x=x+y 这种替换操作使用x.add_(y)\n",
    "result = torch.empty(5,3)\n",
    "torch.add(x,y,out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3776,  0.0080,  1.3954],\n",
      "        [ 0.5815, -0.2561,  2.3722],\n",
      "        [ 0.9675,  0.0134, -1.4088],\n",
      "        [-1.5676, -0.4853, -0.8766],\n",
      "        [-0.3781,  0.1249,  0.4319]])\n",
      "tensor([0.6224, 1.0080, 2.3954])\n",
      "tensor([0.6224, 1.0080, 2.3954])\n"
     ]
    }
   ],
   "source": [
    "#索引结果与源数据共享内存，一个改了另外一个也会变\n",
    "print(x)\n",
    "y = x[0,:]\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "tensor([[ 0.6224,  1.0080,  2.3954],\n",
      "        [ 0.5815, -0.2561,  2.3722],\n",
      "        [ 0.9675,  0.0134, -1.4088],\n",
      "        [-1.5676, -0.4853, -0.8766],\n",
      "        [-0.3781,  0.1249,  0.4319]])\n",
      "tensor([[-0.3776,  0.0080,  1.3954, -0.4185, -1.2561],\n",
      "        [ 1.3722, -0.0325, -0.9866, -2.4088, -2.5676],\n",
      "        [-1.4853, -1.8766, -1.3781, -0.8751, -0.5681]])\n",
      "tensor([[-0.3776,  0.0080,  1.3954],\n",
      "        [-0.4185, -1.2561,  1.3722],\n",
      "        [-0.0325, -0.9866, -2.4088],\n",
      "        [-2.5676, -1.4853, -1.8766],\n",
      "        [-1.3781, -0.8751, -0.5681]])\n"
     ]
    }
   ],
   "source": [
    "#view\n",
    "y = x.view(-1,5)#设定-1表示该位会随着其他设置自行调整\n",
    "print(y.shape)\n",
    "#view也是共享数据，其中一个修改另一个也会改变\n",
    "print(x)\n",
    "y -= 1\n",
    "print(y)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3776, -1.9920, -0.6046],\n",
      "        [-2.4185, -3.2561, -0.6278],\n",
      "        [-2.0325, -2.9866, -4.4088],\n",
      "        [-4.5676, -3.4853, -3.8766],\n",
      "        [-3.3781, -2.8751, -2.5681]])\n",
      "tensor([[-1.3776, -0.9920,  0.3954],\n",
      "        [-1.4185, -2.2561,  0.3722],\n",
      "        [-1.0325, -1.9866, -3.4088],\n",
      "        [-3.5676, -2.4853, -2.8766],\n",
      "        [-2.3781, -1.8751, -1.5681]])\n",
      "tensor([0.6170])\n",
      "0.6169828176498413\n"
     ]
    }
   ],
   "source": [
    "#如果想不共享内存推荐使用clone,clone可以记录在计算图里，梯度回传到副本也会回传到源\n",
    "x_cp = x.clone().view(-1,3)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)\n",
    "#item 将标量tensor转化为Python number\n",
    "x = torch.rand(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "#pytorch支持超过一百种矩阵操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "#广播机制，不同size的tensor通过复制元素完成运算\n",
    "x = torch.arange(1,3).view(1,2)\n",
    "y = torch.arange(1,4).view(3,1)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x+y)#x把第一行复制三行，y把第一列复制两列 对应相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#y = x+y是会开辟新的内存的，然后将y指向新内存\n",
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "id_before = id(y)\n",
    "y[:] = x+y # 结果通过[:]写进y对应的内存中\n",
    "print(id_before == id(y))\n",
    "#同样的节约内存操作还有：\n",
    "torch.add(x,y,out = y)\n",
    "x.add_(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.]]) [[1. 1. 1. 1. 1.]]\n",
      "tensor([[2., 2., 2., 2., 2.]]) [[2. 2. 2. 2. 2.]]\n",
      "[[2. 2. 2. 2. 2.]] tensor([[2., 2., 2., 2., 2.]])\n",
      "[[3. 3. 3. 3. 3.]] tensor([[3., 3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "#tensor numpy互换\n",
    "a = torch.ones([1,5])\n",
    "b = a.numpy() #tensor->numpy,共享相同内存，一个变化另一个也会变化\n",
    "print(a,b)\n",
    "a += 1 \n",
    "print(a,b)\n",
    "\n",
    "c = torch.from_numpy(b) #numpy->tensor\n",
    "print(b,c)\n",
    "b += 1\n",
    "print(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3., 3., 3.]]) tensor([[4., 4., 4., 4., 4.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-fc8bc2dd8f72>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c = torch.tensor(a)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor(a)\n",
    "a+=1\n",
    "print(c,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "tensor([2, 3], device='cuda:0')\n",
      "tensor([2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#tensor on GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(x)\n",
    "    device = torch.device(\"cuda\")\n",
    "    cudax = torch.ones_like(x,device = device) #直接创建一个GPU说的Tensor\n",
    "    x = x.to(device)#把x移到cuda\n",
    "    z = x+cudax\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\",torch.double)) #to还能改数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7ff27d861c70>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#自动求梯度\n",
    "x = torch.ones(2,2,requires_grad=True)#requires_grad默认False，可以通过x.requires_grad(True)设置\n",
    "print(x)\n",
    "print(x.grad_fn) #x不是通过运算得到的所以返回None\n",
    "y = x+2\n",
    "print(y)\n",
    "print(y.grad_fn)#y是通过运算得到的所以返回相关对象\n",
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()#out是标量所以不需要传值进backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n",
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "#梯度是累加的所以再进行反向传播时记得梯度清零\n",
    "x.grad.data.zero_()\n",
    "#不允许张量对张量求导只能标量对张量求导\n",
    "x = torch.tensor([1.0,2.0,3.0,4.0],requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view([2,2])\n",
    "print(z)\n",
    "v = torch.tensor([[1.0,0.1],[0.01,0.001]],dtype=torch.float) #要传入一个和z同形的权重向量进行加权求和\n",
    "z.backward(v) #相当于先计算l = torch.sum(y * v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "#中断梯度追踪\n",
    "x = torch.tensor(1.0,requires_grad=True)\n",
    "y1 = x**2\n",
    "with torch.no_grad():\n",
    "    y2 = x**3\n",
    "y3 = y1 + y2\n",
    "y3.backward()\n",
    "print(x.grad)\n",
    "#因为y2中断梯度所以y2,requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "#如果我们想要修改tensor的数值，但是又不希望被autograd记录（即不会影响反向传播），那么我么可以对tensor.data进行操作。\n",
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
